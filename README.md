ğŸ¤– Gerador de Respostas com Modelos de Linguagem (Falcon 7B & Zephyr 7B)
Este projeto permite carregar modelos de linguagem poderosos da Hugging Face (como o Falcon 7B Instruct e o Zephyr 7B Beta) e gerar respostas a partir de prompts fornecidos pelo usuÃ¡rio. A interaÃ§Ã£o pode ser feita via terminal ou atravÃ©s de uma interface com Gradio (dependendo da versÃ£o do script usada).

ğŸš€ Modelos Suportados
tiiuae/falcon-7b-instruct

HuggingFaceH4/zephyr-7b-beta

ğŸ“¦ Requisitos
Python 3.8+

torch

transformers

gradio (opcional)

Instale as dependÃªncias com:

bash
Copiar
Editar
pip install torch transformers gradio
âš™ï¸ Como Usar
1. Modo Interativo via Terminal (Zephyr 7B)
Este script carrega o modelo Zephyr-7B-Beta e permite enviar prompts diretamente pelo terminal:

bash
Copiar
Editar
python zephyr_terminal.py
Exemplo de uso:
bash
Copiar
Editar
Digite seu prompt (ou 'sair' para encerrar): O que Ã© aprendizado de mÃ¡quina?
Resposta gerada:
Aprendizado de mÃ¡quina Ã© um campo da inteligÃªncia artificial que...
2. Modo Interface com Gradio (Falcon 7B)
Uma interface web interativa pode ser implementada com gr.Interface (veja gradio_falcon.py, por exemplo). O cÃ³digo jÃ¡ inclui carregamento do Falcon 7B Instruct e pode ser adaptado assim:

python
Copiar
Editar
import gradio as gr

def interface_gradio():
    model, tokenizer = carregar_modelo("tiiuae/falcon-7b-instruct")

    def responder(pergunta):
        return gerar_resposta(pergunta, model, tokenizer)

    gr.Interface(fn=responder, inputs="text", outputs="text").launch()

interface_gradio()
ğŸ§  FunÃ§Ãµes Principais
carregar_modelo(model_name): Carrega o modelo e tokenizer, com suporte automÃ¡tico para GPU se disponÃ­vel.

gerar_resposta(prompt, model, tokenizer): Gera uma resposta para o prompt fornecido.

main(): Loop de entrada para interaÃ§Ã£o com o usuÃ¡rio via terminal.

ğŸ’» Dispositivos Suportados
O cÃ³digo detecta automaticamente se hÃ¡ uma GPU disponÃ­vel e utiliza float16 quando aplicÃ¡vel para acelerar a inferÃªncia.

ğŸ› ï¸ Problemas Conhecidos
Alguns modelos podem exigir mais de 16 GB de VRAM para carregar com eficiÃªncia.

Certifique-se de que seu ambiente Python Ã© compatÃ­vel com a versÃ£o de torch e transformers necessÃ¡ria.
